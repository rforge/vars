\SweaveOpts{prefix.string=graph}  
%
% Preambel
%
\documentclass[a4paper]{article}
%
\title{Using the vars package}
\author{Dr. Bernhard Pfaff, Kronberg im Taunus}
%
% Bibliographystle
\usepackage{harvard}
\citationstyle{agsm}
\bibliographystyle{agsm}
\harvardyearparenthesis{square}
\usepackage{amsmath}
%
% Begin of document
%
\begin{document}
%
% Instructions for vignette
% \VignetteIndexEntry{Using the vars package}
% \VignetteDepends{MASS, strucchange}
%\VignetteKeywords{VAR, SVAR, lag selection, diagnsotics, forecasting, causality , FEVD and IRA.}
%\VignettePackage{vars}
\maketitle
%
\abstract{The utilization of the functions contained in the package `vars' are explained by employing a data set of the Canadian economy. The package's scope includes functions for estimating vector autoregressive (henceforth: VAR) and structural vector autoregressive models (henceforth: SVAR). In addition to the two cornerstone functions \texttt{VAR()} and \texttt{SVAR()} for estimating such models, functions for diagnostic testing, estimation of restricted VARs, prediction of VARs, causality analysis, impulse response analysis (henceforth: IRA) and forecast error variance decomposition (henceforth: FEVD) are provided too. In each section, the underlying concept and/or method is briefly explained, thereby drawing on the exhibitions in \citeasnoun{LUE2006}, \citeasnoun{LUE2004}, \citeasnoun{LUE1997}, \citeasnoun{HAM1994} and \citeasnoun{WAT1994}.\\
\par
The package's code is purely written in \texttt{R} and S3-classes with methods have been utilized. It is shipped with a NAMESPACE and a ChangeLog file. It has dependencies to \texttt{MASS} (see \citeasnoun{MASS}) and \texttt{strucchange} (see \citeasnoun{strucchange}).} 
%
\tableofcontents
% 1st Sweave
<<preliminaries, echo=false, results=hide>>= 
library(vars)
library(urca)
library(xtable)
@
%
%
%
\section{Introduction}
\label{sec:intro}
%
\par
Since the critique of \citeasnoun{SIM1980} in the early eighties of the last century, VAR analysis has evolved as a standard instrument in econometrics for analyzing multivariate time series. Because statistical tests are highly used in determining interdependencies and dynamic relationships between variables, it became soon evident to enrich this methodology by incorporating non-statistical \emph{a priori} information, hence SVAR models evolved which try to bypass these shortcomings. At the same time as Sims jeopardized the paradigm of multiple structural equation models laid out by the Cowles Foundation in the forties and fifties of the last century, \citeasnoun{GRA1981} and \citeasnoun{ENG1987} endowed econometricians with a powerful tool for modeling and testing economic relationships, namely, the concept of integration and and co-integration. Nowadays these traces of research are unified in the form of vector error correction and structural vector error correction models. Although these latter topics are left out in this vignette, the interested reader is referred to the monographes of \citeasnoun{LUE2006}, \citeasnoun{HEN1995}, \citeasnoun{JOH1995}, \citeasnoun{HAM1994}, \citeasnoun{BAN1993} and \citeasnoun{urca} for an exhibition of unit root tests and co-integration analysis in \texttt{R}.\\
\par
To the author's knowledge, currently only functions in the base distribution of \texttt{R} and in the \texttt{CRAN}-packages \texttt{dse} (package bundle, see \citename{dse1} \citeyear*{dse3}, \citeyear*{dse2}, \citeyear*{dse1}) and \texttt{fMultivar} \cite{fMultivar} are made available for estimating ARIMA and VARIMA time series models. Though, the \texttt{CRAN}-package \texttt{MSBVAR} \cite{MSBVAR} provides methods for estimating frequentest and Bayesian Vector Auto-regression (BVAR) models, the methods and functions provided in the package \texttt{vars} try to fill a gap in the econometrics' methods landscape of \texttt{R} by providing the `standard' tools in the context of VAR and SVAR analysis.\\
\par
The vignette is structured as follows: the next section is entirely devoted to VARs (definition, estimation, restrictions, diagnostic testing, forecasting, IRA and FEVD). In the ensuing section the topic of SVAR is elucidated (definition, estimation, IRA and FEVD). Finally, the transformation of a VECM to a VAR in levels is exhibited with its associated methods and functions. 
%
%
%
\section{VAR: Vector autoregressive models}
\label{sec:var}
%
\subsection{Definition}
\label{subsec:var-def}
%
In its basic form, a VAR consists of a set of $K$ endogenous variables $\mathbf{y}_t = (y_{1t}, \ldots, y_{kt}, \ldots, y_{Kt})$ for $k = 1, \ldots K$. The VAR(p)-process is then defined as:
%
\begin{equation}
\label{eq1}
\mathbf{y}_t = A_1 \mathbf{y}_{t-1} + \ldots + A_p \mathbf{y}_{t-p} + \mathbf{u}_t \quad ,
\end{equation}
%
with $A_i$ are $(K \times K)$ coefficient matrices for $i = 1, \ldots, p$ and $\mathbf{u}_t$ is a K-dimensional white noise process with time invariant positive definite covariance matrix $E(\mathbf{u}_t \mathbf{u}_t') = \Sigma_{\mathbf{u}}$.\footnote{Vectors are assigned by small bold letters and matrices by capital letters. Scalars are written out as small letters. which are possibly subscripted.}\\
\\par
One important characteristic of a VAR(p)-process is its stability. This means that it generates stationary time series with time invariant means, variances and covariance structure, given sufficient starting values. One can check this by evaluating the reverse characteristic polynomial:
%
\begin{equation}
\det(I_K - A_1 z - \ldots - A_p z^p) \neq 0 \quad \hbox{for} \; |z| \le 1 \;.
\end{equation} 
%
If the solution of the above equation has a root for $z = 1$, then either some or all variables in the VAR(p)-process are integrated of order one, \emph{i.e.} $I(1)$. It might be the case, that co-integration between the variables does exist. This instance can be better analyzed in the context of a vector-error-correction model (VECM). The reader is referred to the monograph of \citeasnoun{JOH1995} for a theoretical exposition and to \citeasnoun{urca} for an implementation with the \texttt{CRAN}-package `urca' in \texttt{R}.\\
\par
In practice, the stability of an empirical VAR(p)-process can be analyzed by considering the companion form and calculating the \emph{eigenvalues} of the coefficient matrix. A VAR(p)-process can be written as a VAR(1)-process as:
%
\begin{equation}
\mathbf{\xi}_t = A \mathbf{\xi}_{t-1} + \mathbf{v}_t \; ,
\end{equation}
%
with:
%
\begin{equation}
\mathbf{\xi}_t =  
\begin{bmatrix}
\mathbf{y}_t \\
\vdots \\
\mathbf{y}_{t-p+1}
\end{bmatrix}
\; , \;
A = 
\begin{bmatrix}
A_1    & A_2    & \cdots & A_{p-1} & A_{p}  \\
I      & 0      & \cdots & 0       & 0      \\
0      & I      & \cdots & 0       & 0      \\
\vdots & \vdots & \ddots & \vdots  & \vdots \\
0      & 0      & \cdots & I       & 0
\end{bmatrix}
\; , \;
\mathbf{v}_t = 
\begin{bmatrix}
\mathbf{u}_t \\
\mathbf{0} \\
\vdots \\
\mathbf{0}
\end{bmatrix}
\; ,
\end{equation}
%
whereby the dimensions of the stacked vectors $\mathbf{\xi}_t$ and $\mathbf{v}_t$ is $(Kp \times 1)$ and of the matrix $A$ is $(Kp \times Kp)$. If the moduli of the \emph{eigenvalues} of $A$ are less than one, then the VAR(p)-process is stable. The calculation of the \emph{eigenvalues} is made available with the function \texttt{roots()}. The function has an argument \texttt{`modulus'} of type logical that returns by default the moduli of the \emph{eigenvalues}, otherwise a vector of complex numbers is returned (for an application see section \ref{subsec:var-est} below).   
%
\subsection{Estimation}
\label{subsec:var-est}
%
For a given sample of the endogenous variables $\mathbf{y}_1, \ldots \mathbf{y}_T$ and sufficient presample values $\mathbf{y}_{-p+1}, \ldots, \mathbf{y}_0$, the coefficients of a VAR(p)-process can be estimated efficiently by least-squares applied separately to each of the equations.\\
\par
Before considering the implementation in the package `vars', let us briefly discuss the utilized data set and plot the series (see figure \ref{fig-canada}). The original time series are published by the OECD and the transformed series, as described in the help page to \texttt{Canada} are provided with \texttt{JMulti} (see \citeasnoun{LUE2004}). The sample range is from the 1stQ 1980 until 4thQ 2000.
%
% 2nd Sweave
<<data, echo=TRUE, results=hide>>= 
library(vars)
data(Canada)
layout(matrix(1:4, nrow = 2, ncol = 2))
plot.ts(Canada$e, main="Employment", ylab = "", xlab = "")
plot.ts(Canada$prod, main="Productivity", ylab = "", xlab = "")
plot.ts(Canada$rw, main="Real Wage", ylab = "", xlab = "")
plot.ts(Canada$U, main="Unemployment Rate", ylab = "", xlab = "")
@
% 
The variable \texttt{e} is used for employment; \texttt{prod} is a measure of labor productivity; \texttt{rw} assigns the real wage and finally \texttt{U} is the unemployment rate.
%
\begin{figure}[Ht]
\centering
<<Canada, echo=FALSE, fig=TRUE>>= 
layout(matrix(1:4, nrow = 2, ncol = 2))
plot.ts(Canada$e, main="Employment", ylab = "", xlab = "")
plot.ts(Canada$prod, main="Productivity", ylab = "", xlab = "")
plot.ts(Canada$rw, main="Real Wage", ylab = "", xlab = "")
plot.ts(Canada$U, main="Unemployment Rate", ylab = "", xlab = "")
@ 
\caption{Canada: Macroeconomic series}
\label{fig-canada}
\end{figure}
%
The function for estimating a VAR is \texttt{VAR()}. It consists of three arguments: the data matrix object \texttt{y} (or an object that can be coerced to a \texttt{matrix}), the integer lag-order \texttt{p} and the type of deterministic regressors to include into the VAR(p). An optimal lag-order can be determined according to an information criteria or the final prediction error of a VAR(p) with the function \texttt{VARselect()}. Its arguments are exhibited in the code snippet below.
%
<<VAR1, echo=TRUE>>=
args(VAR)
args(VARselect)
VARselect(Canada, lag.max = 5, type = "const") 
@ 
%
The \texttt{VARselect()} enables the user to determine an optimal lag length according to an information criteria or the final prediction error of an empirical VAR(p)-process. Each of these measures are defined in the function's help file. The function returns a list object with the optimal lag-order according to each of the criteria, as well as a matrix containing the values for all lags up to \texttt{lag.max}. According to the more conservative $SC(n)$ and $HQ(n)$ criteria, the empirical optimal lag-order is 2. Please note, that the calculation of these criteria is based upon the same sample size, and hence the criteria might take slightly different values whence a VAR for the chosen order is estimated.\\
\par
In a next step, the VAR(2) is estimated with the function \texttt{VAR()} and as deterministic regressors a constant is included.
%
<<VAR2, echo = TRUE>>=
var.2c <- VAR(Canada, p = 2, type = "const")
names(var.2c)
@   
%
The function returns a list object of class \texttt{varest} and the list elements are explained in detail in the function's help file. Let us now focus on two methods, namely \texttt{summary} and \texttt{plot}. 
%
<<VAR3, echo = TRUE, fig=FALSE, results = hide>>=
summary(var.2c)
plot(var.2c)
@ 
%
The \texttt{summary} method simply applies the \texttt{summary.lm} method to the \texttt{lm} objects contained in \texttt{varresult}. The OLS results of the example are shown in separate tables \ref{var.2c.e} -- \ref{var.2c.U} below. It turns out, that not all lagged endogenous variables enter significantly into the equations of the VAR(2). The estimation of a restricted VAR is the topic of the next section.\\ 
%
<<VARsum.e, echo = FALSE, results = tex>>=
xtable(summary(var.2c)[[1]], caption = "Regression result for employment equation", label = "var.2c.e")
@
%
\begin{figure}[Ht]
\centering
<<echo = FALSE, fig = TRUE>>= 
layout(matrix(c(1, 1, 2, 2, 3, 4), nrow = 3, ncol = 2, byrow = TRUE))
plot.ts(var.2c$datamat[, 1], main = paste("Diagram of fit for", colnames(var.2c$datamat)[1], sep=" "), ylim = c(min(c(var.2c$datamat[, 1], var.2c$varresult[[1]]$fitted.values)), max(c(var.2c$datamat[, 1], var.2c$varresult[[1]]$fitted.values))), ylab = "", lty = 1)
lines(var.2c$varresult[[1]]$fitted.values, col = "blue", lty = 2)
plot.ts(var.2c$varresult[[1]]$residuals, main = "Residuals", ylab = "", lty = 1)
abline(h = 0, col = "red")
acf(var.2c$varresult[[1]]$residuals, main = "ACF Residuals", ylab = "")
pacf(var.2c$varresult[[1]]$residuals, main = "PACF Residuals", ylab = "")
@ 
\caption{Employment equation: Diagram of fit, residuals with ACF and PACF}
\label{fig-e}
\end{figure}
%
<<VARsum.prod, echo = FALSE, results = tex>>=
xtable(summary(var.2c)[[2]], caption = "Regression result for productivity equation", label = "var.2c.prod")
@
\par
Before we proceed with the estimation of restricted VARs, let us first look at the \texttt{plot} method for objects with class attribute \texttt{varest} and the \texttt{roots()} function for checking the VARs stability, that was briefly mentioned at the end of the previous section. For each equation in a VAR, a plot consisting of a diagram of fit, a residual plot, the autocorrelation and partial autocorrelation function of the residuals are shown. If the plot method is called interactively, the user is requested to enter \texttt{<RETURN>} for commencing to the next plot. Currently, the \dots-argument of the \texttt{plot} method is unused. However, given the information contained in an object of class \texttt{varest}, it is fairly easy to set up such plots and tailor made them to her/his needs. The plots of the four equations are shown in the exhibits \ref{fig-e} to \ref{fig-U}.
%
\begin{figure}[Ht]
\centering
<<echo = FALSE, fig = TRUE>>= 
layout(matrix(c(1, 1, 2, 2, 3, 4), nrow = 3, ncol = 2, byrow = TRUE))
plot.ts(var.2c$datamat[, 2], main = paste("Diagram of fit for", colnames(var.2c$datamat)[2], sep=" "), ylim = c(min(c(var.2c$datamat[, 2], var.2c$varresult[[2]]$fitted.values)), max(c(var.2c$datamat[, 2], var.2c$varresult[[2]]$fitted.values))), ylab = "", lty = 1)
lines(var.2c$varresult[[2]]$fitted.values, col = "blue", lty = 2)
plot.ts(var.2c$varresult[[2]]$residuals, main = "Residuals", ylab = "", lty = 1)
abline(h = 0, col = "red")
acf(var.2c$varresult[[2]]$residuals, main = "ACF Residuals", ylab = "")
pacf(var.2c$varresult[[2]]$residuals, main = "PACF Residuals", ylab = "")
@ 
\caption{Productivity equation: Diagram of fit, residuals with ACF and PACF}
\label{fig-prod}
\end{figure}
%
<<VARsum.rw, echo = FALSE, results = tex>>=
xtable(summary(var.2c)[[3]], caption = "Regression result for real wage equation", label = "var.2c.rw")
@
%
\begin{figure}[Ht]
\centering
<<echo = FALSE, fig = TRUE>>= 
layout(matrix(c(1, 1, 2, 2, 3, 4), nrow = 3, ncol = 2, byrow = TRUE))
plot.ts(var.2c$datamat[, 3], main = paste("Diagram of fit for", colnames(var.2c$datamat)[3], sep=" "), ylim = c(min(c(var.2c$datamat[, 3], var.2c$varresult[[3]]$fitted.values)), max(c(var.2c$datamat[, 3], var.2c$varresult[[3]]$fitted.values))), ylab = "", lty = 1)
lines(var.2c$varresult[[3]]$fitted.values, col = "blue", lty = 2)
plot.ts(var.2c$varresult[[3]]$residuals, main = "Residuals", ylab = "", lty = 1)
abline(h = 0, col = "red")
acf(var.2c$varresult[[3]]$residuals, main = "ACF Residuals", ylab = "")
pacf(var.2c$varresult[[3]]$residuals, main = "PACF Residuals", ylab = "")
@ 
\caption{Real wage equation: Diagram of fit, residuals with ACF and PACF}
\label{fig-rw}
\end{figure}
%
<<VARsum.U, echo = FALSE, results = tex>>=
xtable(summary(var.2c)[[4]], caption = "Regression result for unemployment equation", label = "var.2c.U")
@
%
\begin{figure}[Ht]
\centering
<<echo = FALSE, fig = TRUE>>= 
layout(matrix(c(1, 1, 2, 2, 3, 4), nrow = 3, ncol = 2, byrow = TRUE))
plot.ts(var.2c$datamat[, 4], main = paste("Diagram of fit for", colnames(var.2c$datamat)[4], sep=" "), ylim = c(min(c(var.2c$datamat[, 4], var.2c$varresult[[4]]$fitted.values)), max(c(var.2c$datamat[, 4], var.2c$varresult[[4]]$fitted.values))), ylab = "", lty = 1)
lines(var.2c$varresult[[4]]$fitted.values, col = "blue", lty = 2)
plot.ts(var.2c$varresult[[4]]$residuals, main = "Residuals", ylab = "", lty = 1)
abline(h = 0, col = "red")
acf(var.2c$varresult[[4]]$residuals, main = "ACF Residuals", ylab = "")
pacf(var.2c$varresult[[4]]$residuals, main = "PACF Residuals", ylab = "")
@ 
\caption{Unemployment equation: Diagram of fit, residuals with ACF and PACF}
\label{fig-U}
\end{figure}
%
Whence, we have estimated a VAR(p), we should check its stability. Here, stability does not refer to the coefficients' stability, \emph{i.e.} the stability of the regressions, but rather the stability of the system of difference equations. As pointed out above, if the moduli of the \emph{eigenvalues} of the companion matrix are less than one, the system is stable. 
%
<<VAR.stable, echo = TRUE>>=
roots(var.2c)
@ 
%
Although, the first \emph{eigenvalue} is pretty close to unity, for the sake of simplicity, we assume a stable VAR(2)-process with a constant as deterministic regressor.
%
\subsection{Restricted VARs}
\label{subsec:var-res}
%
From tables \ref{var.2c.e}-\ref{var.2c.U} it is obvious that not all regressors enter significantly. With the function \texttt{restrict()} the user has the option to re-estimate the VAR either by significance (argument \texttt{method = 'ser'}) or by imposing zero restrictions manually (argument \texttt{method = 'manual'}). In the former case, each equation is re-estimated separately as long as there are t-values that are in absolute value below the threshold value set by the function's argument \texttt{thresh}. In the latter case, a restriction matrix has to be provided that consists of 0/1 values, thereby selecting the coefficients to be retained in the model. The function's arguments are therefore:
%
<<restrict1, echo = TRUE>>=
args(restrict)
## Restrictions by significance
var2c.ser <- restrict(var.2c, method = "ser", thresh = 2.0)
var2c.ser$restrictions
B(var2c.ser)
## Restrictions set manually for third and fourth coefficient
## in first equation
res <- matrix(rep(1, 36), nrow = 4, ncol = 9)
res[1, 3] <- 0
res[1, 4] <- 0
var2c.man <- restrict(var.2c, method = "manual", resmat = res)
var2c.man$restrictions
B(var2c.man)
@
%  
In the example above, the third and fourth coefficient of the employment equation (\emph{i.e.}, the first equation) are set to zero for \texttt{method = 'manual'}. The function \texttt{restrict()} returns a list object with class attribute \texttt{varest}. The coefficients of these objects can be displayed conveniently by either \texttt{B()} (all coefficients, including deterministic coefficients) or by \texttt{A()} (only coefficients of lagged endogenous variables). The output of the former is shown in the code snippet above. It should be noted at this point, that a restricted VAR is estimated by OLS too, instead of employing the EGLS method (see \citeasnoun{LUE2006} for an exposition).  
%
\subsection{Diagnostic testing}
\label{subsec:var-diag}
%
In package `vars' the functions for diagnostic testing are \texttt{arch()}, \texttt{normality()}, \texttt{serial()} and \texttt{stability()}. The former three functions return a list object with class attribute \texttt{varcheck} for which a \texttt{plot}-method exists. The plots, one for each equation, include a residual plot, an empirical distribution plot and the ACFs and PACFs of the residuals and their squares. The function \texttt{stability()} returns a list object with class attribute \texttt{varstabil}. The function itself is just a wrapper for the function \texttt{efp()} from package \texttt{strucchange}. The first element of the returned list object is itself a list of objects with class attribute \texttt{efp}. Hence, the \texttt{plot}-method for objects of class \texttt{varstabil} just call the \texttt{plot}-method for objects of class \texttt{efp}. Let us now turn to each of these functions in more detail.\\
\par
The implemented tests for heteroscedasticity are the univariate and multivariate ARCH test (see \citeasnoun{ENG1982}, \citeasnoun{HAM1994} and \citeasnoun{LUE2006}). The multivariate ARCH-LM test is based on the following regression (the univariate test can be considered as special case of the exhibition below and is skipped):
%
\begin{equation}
vech(\hat{\mathbf{u}_t}\hat{\mathbf{u}_t}') = \mathbf{\beta}_0 + B_1 vech(\hat{\mathbf{u}}_{t-1}\hat{\mathbf{u}}_{t-1}') + \ldots + B_q vech(\hat{\mathbf{u}}_{t-q}\hat{\mathbf{u}}_{t-q}') + \mathbf{v}_t \quad , 
\end{equation}
%
whereby $\mathbf{v}_t$ assigns a spherical error process and $vech$ is the column-stacking operator for symmetric matrices that stacks the columns from the main diagonal on downward. The dimension of $\mathbf{\beta}_0$ is $\frac{1}{2}K(K +1)$ and for the coefficient matrices $B_i$ with $i=1, \ldots, q$, $\frac{1}{2}K(K +1) \times \frac{1}{2}K(K +1)$. The null hypothesis is: $H_0 := B_1 = B_2 = \ldots = B_q = 0$ and the alternative is: $H_1: B_1 \neq 0 \cap B_2 \neq 0 \cap \ldots \cap B_q \neq 0$. The test statistic is defined as:
%
\begin{equation}
VARCH_{LM}(q) = \frac{1}{2}T K (K + 1)R_m^2 \quad ,
\end{equation}
%
with
%
\begin{equation}
R_m^2 = 1 - \frac{2}{K(K+1)}tr(\hat{\Omega} \hat{\Omega}_0^{-1}) \quad ,
\end{equation}
%
and $\hat{\Omega}$ assigns the covariance matrix of the above defined regression model. This test statistic is distributed as $\chi^2(qK^2(K+1)^2/4)$.\\
In the code example below this test is applied to the \texttt{var.2c} object.
<<diag0, echo = TRUE>>=
args(arch)
var2c.arch <- arch(var.2c)
names(var2c.arch)
var2c.arch
@ 
%
\par
The Jarque-Bera normality tests for univariate and multivariate series are implemented and applied to the residuals of a VAR(p) as well as separate tests for multivariate skewness and kurtosis (see \citeasnoun{BER1980}, [\citeyear*{BER1981}] and \citeasnoun{JAR1987} and \citeasnoun{LUE2006}). The univariate versions of the Jarque-Bera test are applied to the residuals of each equation. A multivariate version of this test can be computed by using the residuals that are standardized by a Choleski decomposition of the variance-covariance matrix for the centered residuals. Please note, that in this case the test result is dependent upon the ordering of the variables.
% 
<<diag1, echo = TRUE>>=
var2c.norm <- normality(var.2c, multivariate.only = TRUE)
names(var2c.norm)
var2c.norm
plot(var2c.norm)
@ 
The test statistics for the multivariate case are defined as:
%
\begin{equation}
JB_{mv} = s_3^2 + s_4^2 \; ,
\end{equation}
%
whereby $s_3^2$ and $s_4^2$ are computed according to:
%
\begin{subequations}
\begin{align}
s_3^2 &= T \mathbf{b}_1'\mathbf{b}_1 / 6\label{1st} \\
s_4^2 &= T( \mathbf{b}_2 - \mathbf{3}_K)'(\mathbf{b}_2 - \mathbf{3}_k) / 24 \; ,\label{2nd}
\end{align}
\end{subequations}
%
with $\mathbf{b}_1$ and $\mathbf{b}_2$ are the third and fourth non-central moment vectors of the standardized residuals $\hat{\mathbf{u}}_t^s = \tilde{P}^{-}(\hat{\mathbf{u}}_t - \bar{\hat{\mathbf{u}}}_t)$ and $\tilde{P}$ is a lower triangular matrix with positive diagonal such that $\tilde{P}\tilde{P}' = \tilde{\Sigma}_{\mathbf{u}}$, \emph{i.e.}, the Choleski decomposition of the residual covariance matrix. The test statistic $JB_{mv}$ is distributed as $\chi^2(2K)$ and the multivariate skewness, $s_3^2$, and kurtosis test, $s_4^2$ are distributed as $\chi^2(K)$. Likewise to ARCH, these tests are returned in the list elements \texttt{jb.uni} and \texttt{jb.mul}, which consist of objects with class attribute \texttt{htest}. Hence, the \texttt{print}-method for these objects is employed in \texttt{print.varcheck}. In the code example above, the null hypothesis of normality cannot be rejected.\\
\par
For testing the lack of serial correlation in the residuals of a VAR(p), a Portmanteau test and the LM test proposed by Breusch \& Godfrey are implemented in the function \texttt{serial()}. For both tests small sample modifications are calculated too, whereby the modification for the LM has been introduced by \citeasnoun{EDG1999}. Likewise, to the function \texttt{normality()}, the test statistics are list elements of the returned object and have class attribute \texttt{htest}.\\
The Portmanteau statistic is defined as:
%
\begin{equation}
Q_h = T \sum_{j=1}^h tr(\hat{C}_j'\hat{C}_0^{-1}\hat{C}_j\hat{C}_0^{-1}) \; ,
\end{equation}
%
with $\hat{C}_i = \frac{1}{T}\Sigma_{t=i+1}^T \hat{\mathbf{u}}_t \hat{\mathbf{u}}_{t-i}'$. The test statistic has an approximate $\chi^2(K^2h-n^*)$ distribution, and $n^*$ is the number of coefficients excluding deterministic terms of a VAR(p). The limiting distribution is only valid for $h$ tending to infinity at suitable rate with growing sample size. Hence, the trade-off is between a decent approximation to the $\chi^2$ distribution and a loss in power of the test, when $h$ is chosen too large. The small sample properties of the test statistic:
%
\begin{equation}
Q_h^* = T^2 \sum_{j=1}^h \frac{1}{T-j} tr(\hat{C}_j'\hat{C}_0^{-1}\hat{C}_j\hat{C}_0^{-1})
\end{equation}
%
may be better, and is available as the second entry in the list element \texttt{pt.mul}.\\  
%
<<diag2, echo = TRUE>>=
var2c.pt.asy <- serial(var.2c, lags.pt = 16, type = "PT.asymptotic")
var2c.pt.asy
var2c.pt.adj <- serial(var.2c, lags.pt = 16, type = "PT.adjusted")
var2c.pt.adj
plot(var2c.pt.asy)
@
%
\par
The Breusch-Godfrey LM-statistic \citeaffixed{BRE1978,GOD1978}{see} is based upon the following auxiliary regressions:
%
\begin{equation}
\hat{\mathbf{u}}_t = A_1 \mathbf{y}_{t-1} + \ldots + A_p\mathbf{y}_{t-p} + CD_t + B_1\hat{\mathbf{u}}_{t-1} + \ldots + B_h\hat{\mathbf{u}}_{t-h} + \mathbf{\varepsilon}_t \;.
\end{equation}
%
The null hypothesis is: $H_0: B_1 = \cdots = B_h = 0$ and correspondingly the alternative hypothesis is of the form $H_1: \exists B_i \ne 0 \; for \; i = 1, 2, \ldots, h$. The test statistic is defined as:
%
\begin{equation}
LM_h = T(K - tr(\tilde{\Sigma}_R^{-1}\tilde{\Sigma}_e))  \quad ,
\end{equation}
%
where $\tilde{\Sigma}_R$ and $\tilde{\Sigma}_e$ assign the residual covariance matrix of the restricted and unrestricted model, respectively. The test statistic $LM_h$ is distributed as $\chi^2(hK^2)$ and is returned by the function \texttt{serial()} as list element \texttt{LMh} with class attribute \texttt{htest}.
% 
<<diag3, echo = TRUE>>=
var2c.BG <- serial(var.2c, lags.pt = 16, type = "BG")
var2c.BG
var2c.ES <- serial(var.2c, lags.pt = 16, type = "ES")
var2c.ES
@
%
\citeasnoun{EDG1999} proposed a small sample correction, which is defined as:
%
\begin{equation}
LMF_h = \frac{1 - (1 - R_r^2)^{1/r}}{(1 - R_r^2)^{1/r}} \frac{Nr - q}{K m} \; ,
\end{equation}
%
with $R_r^2 = 1 - |\tilde{\Sigma}_e | / |\tilde{\Sigma}_R|$, $r = ((K^2m^2 - 4)/(K^2 + m^2 - 5))^{1/2}$, $q = 1/2 K m - 1$ and $N = T - K - m - 1/2(K - m + 1)$, whereby $n$ is the number of regressors in the original system and $m = Kh$. The modified test statistic is distributed as $F(hK^2, int(Nr - q))$. This test statistic is returned by \texttt{serial()} as list element \texttt{LMFh} and has class attribute \texttt{htest}.\\
\par
The stability of the regression relationships in a VAR(p) can be assessed with the function \texttt{stability()}. An empirical fluctuation process is estimated for each regression by passing the function's arguments to the \texttt{efp()}-function contained in the package \texttt{strucchange}. The function \texttt{stability()}returns a list object with class attribute \texttt{varstabil}. The first element, \texttt{stability}, is itself a list with objects of class \texttt{efp}. For more information about \texttt{efp()} see its help file and the cited literature therein. 
% 
<<diag4, echo = TRUE>>=
args(stability)
var2c.stab <- stability(var.2c, type = "OLS-CUSUM")
names(var2c.stab)
plot(var2c.stab)
@ 
%
In the R-code example above, an OLS-Cusum test is applied to the \texttt{varest} object \texttt{var.2c}. The graphical output is displayed in figures \ref{fig-stab.e}--\ref{fig-stab.U} on the following pages. The null hypothesis of a stable relationship cannot be rejected for neither regression in the VAR. 
%
\begin{figure}[Ht]
\centering
<<echo = FALSE, fig = TRUE>>= 
plot(var2c.stab$stability$e)
@ 
\caption{Employment equation: OLS-Cusum test}
\label{fig-stab.e}
\end{figure}
%
\begin{figure}[Ht]
\centering
<<echo = FALSE, fig = TRUE>>= 
plot(var2c.stab$stability$prod)
@ 
\caption{Productivity equation: OLS-Cusum test}
\label{fig-stab.prod}
\end{figure}
%
\begin{figure}[Ht]
\centering
<<echo = FALSE, fig = TRUE>>= 
plot(var2c.stab$stability$rw)
@ 
\caption{Real wage equation: OLS-Cusum test}
\label{fig-stab.rw}
\end{figure}
%
\begin{figure}[Ht]
\centering
<<echo = FALSE, fig = TRUE>>= 
plot(var2c.stab$stability$U)
@ 
\caption{Unemployment equation: OLS-Cusum test}
\label{fig-stab.U}
\end{figure}
%
\subsection{Causality Analysis}
\label{subsec:var-cause}
%
Often researchers are interested in the detection of causalities between variables. The most common one is the Granger-Causality test \cite{GRA1969}. Incidentally, this test is not suited to test causal relationships in the strict sense, because the possibility of a \emph{post hoc ergo propter hoc} fallacy cannot be excluded. This is true for any so called ``causality test'' in econometrics. It is therefore common practice to say that variables $x$ does \emph{granger-cause} variable $y$ if variable $x$ helps to predict variable $y$. Aside of this test, within the function \texttt{causality()} a Wald-type instantaneous causality test is implemented too. It is characterized by testing for nonzero correlation between the error processes of the cause and effect variables.
%
<<cause1, echo = TRUE>>=
args(causality)
@ 
%
The function \texttt{causality()} has two arguments. The first argument, \texttt{x}, is an object of class \texttt{varest} and the second, \texttt{cause}, is a character vector of the variable names, that are assumed to be causal to the remaining variables in a VAR(p)-process. If this argument is unset, then the variable in the first column of \texttt{x\$y} is used as cause variable and a warning is printed.\\

For both tests the vector of endogenous variables $\mathbf{y}_t$ is split into two sub-vectors $\mathbf{y}_{1t}$ and $\mathbf{y}_{2t}$ with dimensions $(K_1 \times 1)$ and $(K_2 \times 1)$ with $K = K_1 + K_2$. For the rewritten VAR(p):
%
\begin{equation}
\begin{bmatrix}
\mathbf{y}_{1t} \\
\mathbf{y}_{2t}
\end{bmatrix}
 = \sum_{i=1}^p 
\begin{bmatrix}
\mathbf{\alpha}_{11, i} & \mathbf{\alpha}_{12, i} \\
\mathbf{\alpha}_{21, i} & \mathbf{\alpha}_{22, i}
\end{bmatrix}
\begin{bmatrix}
\mathbf{y}_{1,t-i} \\
\mathbf{y}_{2, t-i}
\end{bmatrix}
+ CD_t + 
\begin{bmatrix}
\mathbf{u}_{1t} \\
\mathbf{u}_{2t}
\end{bmatrix}
\; ,
\end{equation}
%
the null hypothesis that the sub-vector $\mathbf{y}_{1t}$ does not Granger-cause $\mathbf{y}_{2t}$, is defined as $\mathbf{\alpha}_{21, i} = 0$ for $i = 1, 2, \ldots, p$. The alternative is: $\exists \mathbf{\alpha}_{21,i} \ne 0$ for $i = 1, 2, \ldots, p$. The test statistic is distributed as $F(p K_1 K_2, KT - n^*)$, with $n^*$ equal to the total number of parameters in the above VAR(p)-process, including deterministic regressors. The null hypothesis for non-instantaneous causality is defined as: $H_0: C \mathbf{\sigma} = 0$, where $C$ is a $(N \times K(K + 1)/2)$ matrix of rank $N$ selecting the relevant co-variances of $\mathbf{u}_{1t}$ and $\mathbf{u}_{2t}$; $\mathbf{\tilde{\sigma}} = vech(\tilde{\Sigma}_u)$. The Wald statistic is defined as:
%
\begin{equation}
\lambda_W = T \tilde{\mathbf{\sigma}}'C'[2 C D_{K}^{+}(\tilde{\Sigma}_u \otimes \tilde{\Sigma}_u) D_{K}^{+'} C']^{-1} C \tilde{\mathbf{\sigma}}  \; ,
\end{equation}
%
hereby assigning the Moore-Penrose inverse of the duplication matrix $D_K$ with $D_{K}^{+}$ and $\tilde{\Sigma}_u = \frac{1}{T} \Sigma_{t=1}^T \hat{\mathbf{u}}_t \hat{\mathbf{u}}_t'$. The duplication matrix $D_K$ has dimension $(K^2 \times \frac{1}{2}K(K + 1))$ and is defined such that for any symmetric $(K \times K)$ matrix $A$, $vec(A) = D_K vech(A)$ holds. The test statistic $\lambda_W$ is asymptotically distributed as $\chi^2(N)$.\\
\par
The function \texttt{causality()} is now applied for investigating if the real wage and productivity is causal to employment and unemployment.
%
<<cause2, echo = TRUE>>=
causality(var.2c, cause = c("rw", "prod"))
@ 
%
The null hypothesis of no Granger-causality from the real wage and labor productivity to employment and unemployment must be rejected; whereas the null hypothesis of non-instantaneous causality cannot be rejected. This test outcome is economically plausible, given the frictions observed in labor markets.  
%
\subsection{Forecasting}
\label{subsec:var-pred}
%
A \texttt{predict}-method for objects with class attribute \texttt{varest} is available. The \texttt{n.ahead} forecasts are computed recursively for the estimated VAR, beginning with $h = 1, 2, \ldots, n.ahead$: 
%
\begin{equation} 
\mathbf{y}_{T+1 | T} = A_1 \mathbf{y}_T  + \ldots + A_p \mathbf{y}_{T+1-p} + C D_{T+1}
\end{equation}
%
<<predict1, echo = TRUE>>=
var.f10 <- predict(var.2c, n.ahead = 10, ci = 0.95)
names(var.f10)
class(var.f10)
plot(var.f10)
fanchart(var.f10)
@ 
%
Beside the function's arguments for the \texttt{varest} object and the \texttt{n.ahead} forecast steps, a value for the forecast confidence interval can be provided too. Its default value is 0.95. The forecast error covariance matrix is given as:
%
\begin{equation*}
Cov \left ( 
\begin{bmatrix}
\mathbf{y}_{T+1} - \mathbf{y}_{T+1|T} \\
\vdots \\
\mathbf{y}_{T+h} - \mathbf{y}_{T+h|T} 
\end{bmatrix}
\right ) = 
\begin{bmatrix}
I          & 0          & \cdots & 0 \\
\Phi_1     & I          &        & 0 \\
\vdots     &            & \ddots & 0 \\
\Phi_{h-1} & \Phi_{h-2} & \ldots & I
\end{bmatrix}
(\Sigma_{\mathbf{u}} \otimes I_h)
\begin{bmatrix}
I          & 0          & \cdots & 0 \\
\Phi_1     & I          &        & 0 \\
\vdots     &            & \ddots & 0 \\
\Phi_{h-1} & \Phi_{h-2} & \ldots & I
\end{bmatrix}'
\end{equation*}
%
and the matrices $\Phi_i$ are the coefficient matrices of the Wold moving average representation of a stable VAR(p)-process:
%
\begin{equation}
\label{eq15}
\mathbf{y}_t = \Phi_0 \mathbf{u}_t + \Phi_1 \mathbf{u}_{t-1} + \Phi_2 \mathbf{u}_{t-2} + \ldots \; ,
\end{equation}
%
with $\Phi_0 = I_K$ and $\Phi_s$ can be computed recursively according to:
%
\begin{equation}
\label{eq16}
\Phi_s = \sum_{j=1}^s \Phi_{s-j} A_j \; \hbox{for} \; s = 1, 2, \ldots , \; ,
\end{equation}
%
whereby $A_j = 0$ for $j > p$.\\

The \texttt{predict}-method does return a list object of class \texttt{varprd} with three elements. The first element, \texttt{fcst}, is a list of matrices containing the predicted values, the lower and upper bounds according to the chosen confidence interval, \texttt{ci} and its size. The second element, \texttt{endog} is a matrix object containing the endogenous variables and the third is the submitted \texttt{varest} object. A \texttt{plot}-method for objects of class \texttt{varprd} does exist as well as a \texttt{fanchart()} function for plotting fan charts as described in \citeasnoun{BRI1998}.
%
<<fanchart1, echo = TRUE>>=
args(fanchart)
@ 
%
The \texttt{fanchart()} function has \texttt{colors} and \texttt{cis} arguments, allowing the user to input vectors of colors and critical values. If these arguments are left \texttt{NULL}, then as defaults a \texttt{heat.map} color scheme is used and the critical values are set from $0.1$ to $0.9$ with a step size of $0.1$. In order to save space, the predict plot for employment and a fan chart for unemployment is shown in the exhibits \ref{fig-e.prd} and \ref{fig-U.prd}, respectively.
%
\begin{figure}[Ht]
\centering
<<echo = FALSE, fig = TRUE>>= 
smpl <- nrow(var.f10$endog)
ynames <- colnames(var.f10$endog)
fcsty <- c(rep(NA, smpl - 1), var.f10$endog[smpl, 1], var.f10$fcst[[1]][, 1])
fcstl <- c(rep(NA, smpl - 1), var.f10$endog[smpl, 1], var.f10$fcst[[1]][, 2])
fcstu <- c(rep(NA, smpl - 1), var.f10$endog[smpl, 1], var.f10$fcst[[1]][, 3])
smply <- c(var.f10$endog[, 1], rep(NA, length(var.f10$fcst[[1]][, 1])))
min.y <- min(na.omit(c(fcsty, fcstl, fcstu, smply)))
max.y <- max(na.omit(c(fcsty, fcstl, fcstu, smply)))               
plot.ts(fcsty, ylab = "", xlab = "", ylim = c(min.y, max.y), main = paste("Forecast of series", ynames[1]), col = "blue", lty = 2)
lines(smply, col = "black", lty = 1)
lines(fcstl, col = "red", lty = 3)
lines(fcstu, col = "red", lty = 3)
abline(v = smpl, col = "grey", lty = 4)
@ 
\caption{Employment: 10-step ahead forecasts with 95\% confidence}
\label{fig-e.prd}
\end{figure}
%
\begin{figure}[Ht]
\centering
<<echo = FALSE, fig = TRUE>>= 
colors <- gray(sqrt(seq(from = 0.05, to = 1, length = 9)))
cis <- seq(0.1, 0.9, by = 0.1)
n.regions <- length(cis)
n.ahead <- nrow(var.f10$fcst[[1]])
e.sample <- nrow(var.f10$endog)
endog <- var.f10$endog
fcst <- NULL 
for(j in 1:n.regions){
fcst[[j]] <- predict(var.f10$model, n.ahead = n.ahead, ci = cis[j])$fcst
}
xx <- seq(e.sample, length.out = n.ahead + 1)
xx <- c(xx, rev(xx))
ymax <- max(c(fcst[[n.regions]][4][[1]][, 3]), endog[, 4])
ymin <- min(c(fcst[[n.regions]][4][[1]][, 2]), endog[, 4])
yy1 <- c(endog[e.sample, 4], fcst[[1]][4][[1]][, 2], rev(c(endog[e.sample, 4], fcst[[1]][4][[1]][, 3])))
plot.ts(c(endog[, 4], rep(NA, n.ahead)), main = paste("Fan chart for variable", colnames(endog)[4]), xlab = "", ylab = "", ylim = c(ymin, ymax))
polygon(xx, yy1, col = colors[1], border = colors[1])
if(n.regions > 1){
for(l in 2:n.regions){
   yyu <- c(endog[e.sample, 4], fcst[[l]][4][[1]][, 3], rev(c(endog[e.sample, 4], fcst[[l-1]][4][[1]][, 3])))
   yyl <- c(endog[e.sample, 4], fcst[[l-1]][4][[1]][, 2], rev(c(endog[e.sample, 4], fcst[[l]][4][[1]][, 2])))
   polygon(xx, yyu, col = colors[l], border = colors[l])
   polygon(xx, yyl, col = colors[l], border = colors[l])
  }
}
@ 
\caption{Unemployment: Fan chart with default settings}
\label{fig-U.prd}
\end{figure}
%
\subsection{Impulse response analysis}
\label{subsec:var-ira}
%
The impulse response analysis is based upon the Wold moving average representation of a VAR(p)-process (see equations \eqref{eq15} and \eqref{eq16} above). It is used to investigate the dynamic interactions between the endogenous variables. The $(i, j)th$ coefficients of the matrices $\Phi_s$ are thereby interpreted as the expected response of variable $y_{i, t+s}$ to a unit change in variable $y_{jt}$. These effects can be cumulated through time $s = 1, 2, \ldots$ and hence one would obtain the cumulated impact of a unit change in variable $j$ to the variable $i$ at time $s$. Aside of these impulse response coefficients, it is often conceivable to use orthogonalized impulse responses as an alternative. This is the case, if the underlying shocks are less likely to occur in isolation, but rather contemporaneous correlation between the components of the error process $\mathbf{u}_t$ are existent, \emph{i.e.}, the off-diagonal elements of $\Sigma_{\mathbf{u}}$ are non-zero. The orthogonalized impulse responses are derived from a Choleski decomposition of the error variance-covariance matrix: $\Sigma_{\mathbf{u}} = P P'$ with $P$ being a lower triangular. The moving average representation can then be transformed to: 
%
\begin{equation}
\mathbf{y}_t = \Psi_0 \mathbf{\varepsilon}_t + \Psi_1 \mathbf{\varepsilon}_{t-1} + \ldots \;, 
\end{equation}
%
with $\mathbf{\varepsilon}_t = P^{-1}\mathbf{u}_t$ and $\Psi_i = \Phi_i P$ for $i = 0, 1, 2, \ldots$ and $\Psi_0 = P$. Incidentally, because the matrix $P$ is lower triangular, it follows that only a shock in the first variable of a VAR(p)-process does exert an influence on all the remaining ones and that the second and following variables cannot have a direct impact on $y_{1t}$. One should bear this in mind whence orthogonal impulse responses are employed (see the code snippet below and figure \ref{fig-irf} for a re-ordering of the data to compute the impulse responses originating from a real wage shock to employment and unemployment). Please note further, that a different ordering of the variables might produce different outcomes with respect to the impulse responses. As we shall see in section \ref{sec:svar}, the non-uniqueness of the impulse responses can be circumvented by analyzing a set of endogenous variables in the structural VAR (SVAR) framework. 
%
<<irf1, echo = TRUE>>=
args(irf)
Canada2 <- Canada[, c(3, 1, 4, 2)]
names(Canada2)
var.2c.alt <- VAR(Canada2, p = 2, type = "const")
irf.rw.eU <- irf(var.2c.alt, impulse = "rw", response = c("e", "U"), boot = TRUE) 
names(irf.rw.eU)
plot(irf.rw.eU)
@  
%
The function for conducting impulse response analysis is \texttt{irf()}. It is a method for objects with class attribute \texttt{varest} and \texttt{svarest}. The function's arguments are displayed in the R-code example above. The impulse variables are set as a character vector \texttt{impulse} and the responses are provided likewise in the argument \texttt{response}. If either one is unset, then all variables are considered as impulses or responses, respectively. The default length of the impulse responses is set to $10$ via argument \texttt{n.ahead}. The computation of orthogonalized and/or cumulated impulse responses is controlled by the logical switches \texttt{ortho} and \texttt{cumulative}, respectively. Finally, confidence bands can be returned by setting \texttt{boot = TRUE} (default). The pre-set values are to run 100 replications and return 95\% confidence bands. It is at the user's leisure to specify a \texttt{seed} for replicability of the  results. The standard percentile interval is calculated as $CI_s = [s_{\gamma/2}^*, s_{(1-\gamma)/2}^*]$, where $s_{\gamma/2}^*$ and $s_{(1-\gamma)/2}^*$ are the $\gamma/2$ and $(1-\gamma)/2$ quantiles of the estimated bootstraped impulse response coefficients $\hat{\Phi}^*$ or $\hat{\Psi}^*$ \cite{EFR1993}. The function \texttt{irf()} returns an object with class attribute \texttt{varirf} for which a \texttt{plot} method does exist.   
%
\begin{figure}[Ht]
\centering
<<echo = FALSE, fig = TRUE>>= 
plot(irf.rw.eU)
@ 
\caption{Orthogonal impulse responses from real wage to employment and unemployment (95\% confidence bands, boots trapped with 100 replications)}
\label{fig-irf}
\end{figure}
%
In figure \ref{fig-irf} the result of the impulse response analysis is depicted. Although a positive real wage shock does influence employment and unemployment in the directions one would \emph{a priori} assume, it does so only significantly for unemployment during the time span ranging from the third period to the sixth period.\footnote{Incidentally, the result should not be mistaken or interpreted as demanding higher real wages, \emph{i.e.}, having a free lunch, without facing the consequences beyond of the model's scope. The example has been primarily chosen for demonstrations purposes only.} 
%
\subsection{Forecast error variance decomposition}
\label{subsec:var-fevd}
%
The forecast error variance decomposition (henceforth: FEVD) is based upon the orthogonalized impulse response coefficient matrices $\Psi_{n}$ (see section \ref{subsec:var-ira} above). The FEVD allows the user to analyze the contribution of variable $j$ to the $h$-step forecast error variance of variable $k$. If the element-wise squared orthogonalized impulse responses are divided by the variance of the forecast error variance, $\sigma_k^2(h)$, the resultant is a percentage figure. Formally, the forecast error variance for $y_{k, T+h} - Y_{k, T+h|T}$ is defined as:
%
\begin{equation}
\sigma_k^2(h) = \sum_{n=0}^{h-1}(\psi_{k1, n}^2 + \ldots + \psi_{kK, n}^2)
\end{equation}
%
which can be written as:
%
\begin{equation}
\sigma_k^2(h) = \sum_{j=1}^K(\psi_{kj, 0}^2 + \ldots + \psi_{kj, h-1}^2) \; .
\end{equation}
%
Dividing the term $(\psi_{kj, 0}^2 + \ldots + \psi_{kj, h-1}^2)$ by $\sigma_k^2(h)$ yields the forecast error variance decompositions in percentage terms:
%
\begin{equation}
\omega_{kj}(h) = (\psi_{kj, 0}^2 + \ldots + \psi_{kj, h-1}^2)/\sigma_k^2(h)\; .
\end{equation}
%
The method \texttt{fevd} is available for conducting FEVD. Functions for objects of classes \texttt{varest} and \texttt{svarest} do exist. The argument aside of an object with class attribute being either \texttt{varest} or \texttt{svarest} is the number of forecasting steps, \texttt{n.ahead}. Its default value has been set to 10. Currently, the $\ldots$-argument is unused.
%
<<fevd1, echo = TRUE>>=
args(fevd)
@  
%
The method does return a list object with class attribute \texttt{varfevd} for which a \texttt{plot}-method does exists. The list elements are the forecast error variances organized on a per-variable basis.
%
<<fevd2, echo = TRUE>>=
var2c.fevd <- fevd(var.2c, n.ahead = 5)
class(var2c.fevd)
names(var2c.fevd)
var2c.fevd$e
@
%
From the example above, it is evident that aside of employment itself, the influence of productivity is contributing the most to the forecast uncertainty of employment. In the exhibit \ref{fig-fevd}, the FEVD of employment for five ahead steps is plotted as a bar graph.\footnote{If the \texttt{plot}-method is called interactively, the user is enforced to go through the bar graphs for all variables. In order to save space, only the FEVD for the employment variable is shown in figure \ref{fig-fevd}.}
%
\begin{figure}[Ht]
\centering
<<fevd3, echo = FALSE, fig = TRUE>>=
barplot(t(var2c.fevd[[1]]), main = paste("FEVD for", names(var2c.fevd)[1]), col = palette()[1 : 4], ylab = "Percentage", xlab = "Horizon", names.arg = paste(1 : nrow(var2c.fevd[[1]])), ylim = c(0, 1.2))
legend("top", legend = names(var2c.fevd), fill = palette()[1 : 4], ncol = 4)
@
\caption{Forecast error variance decomposition for employment}
\label{fig-fevd}
\end{figure}
%
%
%
\section{SVAR: Structural vector autoregressive models}
\label{sec:svar}
%
\subsection{Definition}
\label{subsec:svar-def}
%
Recall from section \ref{subsec:var-def} on page \pageref{subsec:var-def} the definition of a VAR(p)-process, in particular equation \eqref{eq1}. A VAR(p) can be interpreted as a reduced form model. A SVAR-model is its structural form and is defined as: 
%
\begin{equation}
\label{eq21}
A \mathbf{y}_t = A_1^* \mathbf{y}_{t-1} + \ldots + A_p^* \mathbf{y}_{t-p} + B \mathbf{\varepsilon}_t \; .
\end{equation}
%
It is assumed that the \emph{structural errors}, $\mathbf{\varepsilon}_t$, are white noise and the coefficient matrices $A_i^*$ for $i = 1, \ldots, p$, are structural coefficients that might differ from their reduced form counterparts. To see this, consider the resulting equation by left-multiplying equation \eqref{eq21} with the inverse of $A$:
%
\begin{equation}
\label{eq22}
\begin{split}
\mathbf{y}_t & = A^{-1} A_1^* \mathbf{y}_{t-1} + \ldots + A^{-1} A_p^* \mathbf{y}_{t-p} + A^{-1} B \mathbf{\varepsilon}_t \\
\mathbf{y}_t & = A_1 \mathbf{y}_{t-1} + \ldots + A_p \mathbf{y}_{t-p} + \mathbf{u}_t \; .
\end{split}
\end{equation}
%
A SVAR-model can be used to identify shocks and trace these out by employing IRA and/or FEVD through imposing restrictions on the matrices $A$ and/or $B$. Incidentally, though a SVAR-model is a structural model, it departs from a reduced form VAR(p)-model and only restrictions for $A$ and $B$ can be added. It should be noted that the reduced form residuals can be retrieved from a SVAR-model by $\mathbf{u}_t = A^{-1} B \mathbf{\varepsilon}_t$ and its variance-covariance matrix by $\Sigma_{\mathbf{u}} = A^{-1} B B' A^{-1'}$.\\
\par
Depending on the imposed restrictions, three types of SVAR- models can be distinguished:
%
\begin{itemize}
\item A model: $B$ is set to $I_K$ (minimum number of restrictions for identification is $K(K-1)/2$ ).
\item B model: $A$ is set to $I_K$ (minimum number of restrictions to be imposed for identification is the same as for A model).
\item AB model: restrictions can be placed on both matrices (minimum number of restrictions for identification is $K^2 + K(K-1)/2$).
\end{itemize} 
%
\subsection{Estimation}
\label{subsec:svar-est}
%
A SVAR model is estimated with the function
\texttt{SVAR()}.\footnote{The following exhibition uses the estimation method
\texttt{logLik} for directly minimizing the negative
log-likelihood function. It also possible to use a scoring algorithm. Hereby,
the restrictions have to be provided in a different form (see
\texttt{example(SVAR)} for further details.} Its arguments are shown in the
R-code example below. An object with class attribute \texttt{varest}
has to be provided as argument \texttt{x}. Whether an A-, B- or
AB-model will be estimated, is dependent on the setting for
\texttt{Amat} and \texttt{Bmat}. If a restriction matrix for
\texttt{Amat} with dimension $(K \times K)$ is provided and the
argument \texttt{Bmat} is left \texttt{NULL}, an A-model be
estimated. In this case \texttt{Bmat} is set to an identity matrix
$I_K$. Alternatively, if only a matrix object for \texttt{Bmat} is
provided and \texttt{Amat} left unchanged, then a B-model will be
estimated and internally \texttt{Amat} is set to an identity matrix
$I_K$. Finally, if matrix objects for both arguments are provided,
then an AB-model will be estimated. In all cases, the matrix elements
to be estimated are marked by \texttt{NA} entries at the relevant
positions. 
%
<<SVAR1, echo = TRUE>>=
args(SVAR)
amat <- diag(4)
diag(amat) <- NA
amat[1, 2] <- NA
amat[1, 3] <- NA
amat[3, 2] <- NA
amat[4, 1] <- NA
amat
@
%
In the example above, an expository object \texttt{amat} has been set up. The next function's argument is \texttt{start}. This argument expects a vector of starting values, hence, the user has one handle at hand for controlling the optimization process in case one encounters convergence problems. If left \texttt{NULL}, then starting values are set to $0.1$ for all coefficients to be optimized. Finally, the $\ldots$ argument in \texttt{SVAR()} is parsed to the \texttt{optim()} function. Therefore, it is possible to choose the numerical optimization method and set \texttt{hessian = TRUE} to obtain the numerical standard errors, for instance.\\
\par
The parameters are estimated by minimizing the negative of the concentrated log-likelihood function:
%
\begin{equation}
\label{eq23}
\begin{split}
\ln L_c(A, B) = &- \frac{KT}{2}\ln(2\pi) + \frac{T}{2}\ln|A|^2 -\frac{T}{2}\ln|B|^2 \\
\; &- \frac{T}{2}tr(A'B'^{-1}B^{-1}A\tilde{\Sigma}_u) \; ,
\end{split}
\end{equation}
%
by utilizing \texttt{optim}. 
%
<<SVAR2, echo = TRUE>>=
args(optim)
svar2c.A <- SVAR(var.2c, estmethod = "logLik", Amat = amat, Bmat = NULL, 
   hessian = TRUE, method = "BFGS") 
svar2c.A
@ 
%
The returned object of function \texttt{SVAR()} is a list with class attribute \texttt{svarest}.
%
<<SVAR3, echo = TRUE>>=
class(svar2c.A)
names(svar2c.A)
@ 
%
Dependent on the chosen model and if the argument \texttt{hessian = TRUE} has been set, the list elements \texttt{A, Ase, B, Bse} contain the estimated coefficient matrices with the numerical standard errors, if applicable. The element \texttt{LRIM} does contain the long-run impact matrix in case a SVAR of type Blanchard \& Quah is estimated, otherwise this element is \texttt{NULL} (see below for more details). The list element \texttt{Sigma.U} is the variance-covariance matrix of the reduced form residuals times 100, \emph{i.e.}, $\Sigma_U = A^{-1}BB'A^{-1'} \times 100$. The list element \texttt{LR} is an object with class attribute \texttt{htest}, holding the Likelihood ratio over-identification test, whereby the test statistic is defined as:
%
\begin{equation}
\label{eq24}
LR = T ( \log det(\tilde{\Sigma}_{\mathbf{u}}^r) - \log det(\tilde{\Sigma}_{\mathbf{u}})) \;
\end{equation}
%
with $\tilde{\Sigma}_{\mathbf{u}}$ being the ML estimator of the reduced form variance-covariance matrix and $\tilde{\Sigma}_{\mathbf{u}}^r$ is the corresponding estimator obtained from the restricted structural form estimation. The element \texttt{opt} is the returned object from function \texttt{optim}. The remaining four list items are the vector of starting values, the SVAR model type, the \texttt{varest} object and the \texttt{call} to \texttt{SVAR()}.\\
\par
Finally, it should be noted that the model proposed by \citeasnoun{BLA1989} can be considered as a particular SVAR-model of type $B$. The matrix $A$ is equal to $I_K$ and the matrix of the long-run effects is lower-triangular and defined as:
%
\begin{equation}
\label{eq25}
(I_K - A_1 - \cdots - A_P)^{-1}B \quad .
\end{equation}
%
Hence, the residual of the second equation cannot exert a long-run influence on the first variable and likewise the third residual cannot impact the first and second variable. The estimation of the \citename{BLA1989} model is achieved by a Choleski decomposition of:
%
\begin{equation}
\label{eq26}
(I_K - \hat{A}_1 - \cdots - \hat{A}_p)^{-1}\hat{\Sigma}_u (I_K - \hat{A}_1' - \cdots - \hat{A}_p')^{-1}\quad .
\end{equation}  
%
The matrices $\hat{A}_i$ for $i = 1, \ldots, p$ assign the reduced form estimates. The long-run impact matrix is the lower-triangular Choleski decomposition of this matrix and the contemporaneous impact matrix is equal to:
%
\begin{equation}
\label{eq27}
(I_K - A_1 - \cdots - A_p)Q \quad,
\end{equation}
%
where $Q$ assigns the lower-triangular Choleski decomposition. It should be noted that a re-ordering of the VAR might become necessary due to the recursive nature of the model.\\
\par
The \citename{BLA1989} model is implemented as function \texttt{BQ()}. It has one argument $x$, which is an object with class attribute \texttt{varest}. The function does return a list object with class attribute \texttt{svarest}. Below is a trivial example of applying a \citeasnoun{BLA1989}-type SVAR to the already available VAR \texttt{var.2c}.
%
<<svar-bq, echo = TRUE>>=
BQ(var.2c)
@ 
%
The contemporaneous impact matrix is stored as list-element \texttt{B} and the long-run impact matrix as list-element \texttt{LRIM} in the returned object. 
%
\subsection{Impulse response analysis}
\label{subsec:svar-ira}
%
Like impulse response analysis can be conducted for objects with class attribute \texttt{varest} it can be done so for objects with class attribute \texttt{svarest} (see section \ref{subsec:var-ira} on page \pageref{subsec:var-ira} following). In fact, the \texttt{irf}-methods for classes \texttt{varest} and \texttt{svarest} are at hand with the same set of arguments; except \texttt{ortho} is missing for objects of class \texttt{svarest} due to the nature and interpretation of the error terms in a SVAR. The impulse response coefficients for a SVAR are calculated as $\Theta_i = \Phi_i A^{-1}B$ for $i = 1, \ldots, n$.\\
In the code snippet below, an example of an IRA is exhibited for the estimated SVAR from the previous section. 
%
<<svar-irf1, echo = TRUE>>=
svar2cA.ira <- irf(svar2c.A, impulse = "rw", response = c("e", "U"), boot = FALSE)
svar2cA.ira
@ 
%
\subsection{Forecast error variance decomposition}
\label{subsec:svar-fevd}
%
A forecast error variance decomposition can be applied likewise to objects of class \texttt{svarest}. Here, the forecast errors of $y_{T+h|T}$ are derived from the impulse responses of a SVAR and the derivation to the forecast error variance decomposition is similar to the one outlined for VARs (see section \ref{subsec:var-fevd} on page \pageref{subsec:var-fevd} following).\\
An application is provided in the code snippet below and a plot for the real wage equation is exhibited in figure \ref{fig-svarfevd} on page \pageref{fig-svarfevd}.
<<fevd-svar, echo = TRUE, fig = FALSE>>=
svar2cA.fevd <- fevd(svar2c.A , n.ahead = 8)
plot(svar2cA.fevd)
@ 
%
\begin{figure}[Ht]
\centering
<<fevd-svar-fig, echo = FALSE, fig = TRUE>>=
barplot(t(svar2cA.fevd[[3]]), main = paste("FEVD for", names(svar2cA.fevd)[3]), col = palette()[1 : 4], ylab = "Percentage", xlab = "Horizon", names.arg = paste(1 : nrow(svar2cA.fevd[[3]])), ylim = c(0, 1.2))
legend("top", legend = names(svar2cA.fevd), fill = palette()[1 : 4], ncol = 4)
@
\caption{SVAR: FEVD for real wage}
\label{fig-svarfevd}
\end{figure}
%
%
%
\section{VECM to VAR}
\label{sec:vec2var}
%
%
%
Reconsider the VAR presentation from equation \ref{eq1} on page \pageref{eq1}:
\begin{equation}
\label{eq28}
\mathbf{y}_t = A_1 \mathbf{y}_{t-1} + \ldots + A_p \mathbf{y}_{t-p} + \mathbf{u}_t \quad ,
\end{equation}
The following vector error correction specifications do exist, which can be estimated with function \texttt{ca.jo()} contained in \texttt{urca}:
%
\begin{equation}
\label{eq29}
\Delta \mathbf{y}_t = \Pi \mathbf{y}_{t-p} + \Gamma_1 \Delta \mathbf{y}_{t-1} + \ldots + \Gamma_{p-1} \mathbf{y}_{t-p+1} + \mathbf{u}_t \quad ,
\end{equation}
%
with
%
\begin{equation}
\label{eq30}
\Gamma_i = -(I - A_1 - \ldots - A_i), \quad i = 1, \ldots , p-1.
\end{equation}
%
and
%
\begin{equation}
\label{eq31}
\Pi = -(I - A_1 - \ldots - A_p) \quad.
\end{equation}
The $\Gamma_i$ matrices contain the cumulative long-run impacts, hence this VECM specification is signified by long-run form. The other specification is given as follows:
%
\begin{equation}
\label{eq32}
\Delta \mathbf{y}_t = \Pi \mathbf{y}_{t-1} + \Gamma_1 \Delta \mathbf{y}_{t-1} + \ldots + \Gamma_{p-1} \mathbf{y}_{t-p+1} + \mathbf{u}_t \quad,
\end{equation}
with
%
\begin{equation}
\label{eq33}
\Gamma_i = -(A_{i+1} + \ldots + A_p) \quad i = 1, \ldots, p-1.
\end{equation}
Equation \ref{eq31} applies to this specification too. Hence, the $\Pi$  matrix is the same as in the first specification. However, the $\Gamma_i$ matrices now differ, in the sense that they measure transitory effects. Therefore this specification is signified by transitory form.\\
\par
With function \texttt{vec2var()} a VECM (\emph{i.e} an object of formal class \texttt{ca.jo}, generated by \texttt{ca.jo()}) is transformed into its level VAR representation. For illustration purposes, an example taken from \citeasnoun{JOH1990} is used in the following.
%
<<vecm, echo = TRUE, fig = FALSE>>=
library(urca)
data(finland)
sjf <- finland
sjf.vecm <- ca.jo(sjf, ecdet = "none", type = "eigen", K = 2,
spec = "longrun", season = 4)
summary(sjf.vecm)
@ 
%
One can conclude that two co-integration relationships do exist. The co-integrating rank is needed for the above transformation with function \texttt{vec2var}. Given the object \texttt{sjf.vecm} of class \texttt{ca.jo} with $r = 2$, one can now swiftly proceed:
<<vec2var, echo = TRUE, fig = FALSE>>=
args(vec2var)
sjf.var <- vec2var(sjf.vecm, r = 2)
sjf.var
@ 
%
The \texttt{print} method does return the coefficient values, first for the lagged endogenous variables, next for the deterministic regressors. The returned object is of class \texttt{vec2var} and this list has the following elements:
%
<<vec2var2, echo = TRUE, fig = FALSE>>=
names(sjf.var)
class(sjf.var)
methods(class = "vec2var")
@
%
The object has assigned class attribute \texttt{vec2var} and a \texttt{print}- , \texttt{irf}-, \texttt{fevd}-, \texttt{predict}-, \texttt{Phi} and \texttt{Psi}-method do exist. Furthermore, the functions \texttt{arch()}, \texttt{normality()}, \texttt{serial()} and \texttt{fanchart()} can be employed for further analysis too.\footnote{The reader is referred to the previous sections in which these methods and functions are outlined.}\\
\par
Finally, structural vector error correction models can be estimated
with the function \texttt{SVEC()}. Likewise to the function
\texttt{SVAR2()} a scoring algorithm is utilized for determining the
coefficients of the long run and contemporaneous impact matrices. The
estimated standard errors of these parameters can be calculated by a
bootstrapping procedure. The latter is controlled by the functional
argument \texttt{boot} in \texttt{SVEC}; its default value is
\texttt{FALSE}.   
%
\newpage
%
\listoftables
%
\listoffigures
%
% Bibliography
%
\bibliography{vars}
%
\end{document}

